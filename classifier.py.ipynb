{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.applications import MobileNet\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.layers import Dense, Activation, Dropout, GlobalAveragePooling2D\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dongjun/anaconda3/envs/dj/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "CLASSES = 2 # missing pulley or perfect pulley\n",
    "\n",
    "baseModel = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "x = baseModel.output\n",
    "\n",
    "#Test the SVM or DNN\n",
    "# Change the Flatten layer,\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Change the number of Dense layer and number of neuron\n",
    "# Make the relu6 activation function and test it.\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# Change the Dropout ratio\n",
    "x = Dropout(0.5)(x)\n",
    "#x = Dense(1024, activation='relu')(x)\n",
    "#x = Dense(512, activation='relu')(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "preds = Dense(CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=baseModel.input, outputs=preds)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <keras.engine.input_layer.InputLayer object at 0x7f489e940d50>\n",
      "1: <keras.layers.convolutional.ZeroPadding2D object at 0x7f48dc08f1d0>\n",
      "2: <keras.layers.convolutional.Conv2D object at 0x7f48dc0eead0>\n",
      "3: <keras.layers.normalization.BatchNormalization object at 0x7f489e957310>\n",
      "4: <keras.layers.advanced_activations.ReLU object at 0x7f489e111350>\n",
      "5: <keras.layers.convolutional.DepthwiseConv2D object at 0x7f489e12bfd0>\n",
      "6: <keras.layers.normalization.BatchNormalization object at 0x7f489e957490>\n",
      "7: <keras.layers.advanced_activations.ReLU object at 0x7f48947c5610>\n",
      "8: <keras.layers.convolutional.Conv2D object at 0x7f48947c5dd0>\n",
      "9: <keras.layers.normalization.BatchNormalization object at 0x7f48946fcf10>\n",
      "10: <keras.layers.advanced_activations.ReLU object at 0x7f4894738990>\n",
      "11: <keras.layers.convolutional.ZeroPadding2D object at 0x7f4894738d90>\n",
      "12: <keras.layers.convolutional.DepthwiseConv2D object at 0x7f48946eae10>\n",
      "13: <keras.layers.normalization.BatchNormalization object at 0x7f48945fff10>\n",
      "14: <keras.layers.advanced_activations.ReLU object at 0x7f4894620b50>\n",
      "15: <keras.layers.convolutional.Conv2D object at 0x7f48945cd110>\n",
      "16: <keras.layers.normalization.BatchNormalization object at 0x7f4894566650>\n",
      "17: <keras.layers.advanced_activations.ReLU object at 0x7f4894523c10>\n",
      "18: <keras.layers.convolutional.DepthwiseConv2D object at 0x7f4894523ed0>\n",
      "19: <keras.layers.normalization.BatchNormalization object at 0x7f48944b2f10>\n",
      "20: <keras.layers.advanced_activations.ReLU object at 0x7f489440df90>\n",
      "21: <keras.layers.convolutional.Conv2D object at 0x7f489440db10>\n",
      "22: <keras.layers.normalization.BatchNormalization object at 0x7f48943b8450>\n",
      "23: <keras.layers.advanced_activations.ReLU object at 0x7f4894375650>\n",
      "24: <keras.layers.convolutional.ZeroPadding2D object at 0x7f4894375d90>\n",
      "25: <keras.layers.convolutional.DepthwiseConv2D object at 0x7f489430ab90>\n",
      "26: <keras.layers.normalization.BatchNormalization object at 0x7f489425ee10>\n",
      "27: <keras.layers.advanced_activations.ReLU object at 0x7f489420f0d0>\n",
      "28: <keras.layers.convolutional.Conv2D object at 0x7f489420f890>\n",
      "29: <keras.layers.normalization.BatchNormalization object at 0x7f48941a3a10>\n",
      "30: <keras.layers.advanced_activations.ReLU object at 0x7f4894177890>\n",
      "31: <keras.layers.convolutional.DepthwiseConv2D object at 0x7f4894161850>\n",
      "32: <keras.layers.normalization.BatchNormalization object at 0x7f489408c5d0>\n",
      "33: <keras.layers.advanced_activations.ReLU object at 0x7f489404ad50>\n",
      "34: <keras.layers.convolutional.Conv2D object at 0x7f48940766d0>\n",
      "35: <keras.layers.normalization.BatchNormalization object at 0x7f486874f1d0>\n",
      "36: <keras.layers.advanced_activations.ReLU object at 0x7f486874fc50>\n",
      "37: <keras.layers.convolutional.ZeroPadding2D object at 0x7f486870f0d0>\n",
      "38: <keras.layers.convolutional.DepthwiseConv2D object at 0x7f48686a0e50>\n",
      "39: <keras.layers.normalization.BatchNormalization object at 0x7f486865bfd0>\n",
      "40: <keras.layers.advanced_activations.ReLU object at 0x7f486860d510>\n",
      "41: <keras.layers.convolutional.Conv2D object at 0x7f486860dcd0>\n",
      "42: <keras.layers.normalization.BatchNormalization object at 0x7f48685a2550>\n",
      "43: <keras.layers.advanced_activations.ReLU object at 0x7f4868576e10>\n",
      "44: <keras.layers.convolutional.DepthwiseConv2D object at 0x7f486855e750>\n",
      "45: <keras.layers.normalization.BatchNormalization object at 0x7f4868485f50>\n",
      "46: <keras.layers.advanced_activations.ReLU object at 0x7f48684623d0>\n",
      "47: <keras.layers.convolutional.Conv2D object at 0x7f4868462cd0>\n",
      "48: <keras.layers.normalization.BatchNormalization object at 0x7f486838c650>\n",
      "49: <keras.layers.advanced_activations.ReLU object at 0x7f486834bf50>\n",
      "50: <keras.layers.convolutional.DepthwiseConv2D object at 0x7f486834bc10>\n",
      "51: <keras.layers.normalization.BatchNormalization object at 0x7f48682dcfd0>\n",
      "52: <keras.layers.advanced_activations.ReLU object at 0x7f48682b6b90>\n",
      "53: <keras.layers.convolutional.Conv2D object at 0x7f48682b6ed0>\n",
      "54: <keras.layers.normalization.BatchNormalization object at 0x7f48681df490>\n",
      "55: <keras.layers.advanced_activations.ReLU object at 0x7f486819c690>\n",
      "56: <keras.layers.convolutional.DepthwiseConv2D object at 0x7f486819cc90>\n",
      "57: <keras.layers.normalization.BatchNormalization object at 0x7f48680c6b10>\n",
      "58: <keras.layers.advanced_activations.ReLU object at 0x7f48680a3950>\n",
      "59: <keras.layers.convolutional.Conv2D object at 0x7f48680a3690>\n",
      "60: <keras.layers.normalization.BatchNormalization object at 0x7f484878bdd0>\n",
      "61: <keras.layers.advanced_activations.ReLU object at 0x7f484874ab50>\n",
      "62: <keras.layers.convolutional.DepthwiseConv2D object at 0x7f484874ae10>\n",
      "63: <keras.layers.normalization.BatchNormalization object at 0x7f48486dff10>\n",
      "64: <keras.layers.advanced_activations.ReLU object at 0x7f4848646590>\n",
      "65: <keras.layers.convolutional.Conv2D object at 0x7f48486b7dd0>\n",
      "66: <keras.layers.normalization.BatchNormalization object at 0x7f48485e2510>\n",
      "67: <keras.layers.advanced_activations.ReLU object at 0x7f484859f8d0>\n",
      "68: <keras.layers.convolutional.DepthwiseConv2D object at 0x7f484859f9d0>\n",
      "69: <keras.layers.normalization.BatchNormalization object at 0x7f48484cd850>\n",
      "70: <keras.layers.advanced_activations.ReLU object at 0x7f4848487ed0>\n",
      "71: <keras.layers.convolutional.Conv2D object at 0x7f48484a1d10>\n",
      "72: <keras.layers.normalization.BatchNormalization object at 0x7f48483cb590>\n",
      "73: <keras.layers.advanced_activations.ReLU object at 0x7f48483a23d0>\n",
      "74: <keras.layers.convolutional.ZeroPadding2D object at 0x7f484838f6d0>\n",
      "75: <keras.layers.convolutional.DepthwiseConv2D object at 0x7f4848345710>\n",
      "76: <keras.layers.normalization.BatchNormalization object at 0x7f48482d7910>\n",
      "77: <keras.layers.advanced_activations.ReLU object at 0x7f48482af650>\n",
      "78: <keras.layers.convolutional.Conv2D object at 0x7f4848287ed0>\n",
      "79: <keras.layers.normalization.BatchNormalization object at 0x7f48482370d0>\n",
      "80: <keras.layers.advanced_activations.ReLU object at 0x7f48481f63d0>\n",
      "81: <keras.layers.convolutional.DepthwiseConv2D object at 0x7f48481f6c90>\n",
      "82: <keras.layers.normalization.BatchNormalization object at 0x7f4848109d90>\n",
      "83: <keras.layers.advanced_activations.ReLU object at 0x7f48480e5150>\n",
      "84: <keras.layers.convolutional.Conv2D object at 0x7f48480e5690>\n",
      "85: <keras.layers.normalization.BatchNormalization object at 0x7f483e7d4e90>\n",
      "86: <keras.layers.advanced_activations.ReLU object at 0x7f483e7a2750>\n",
      "87: <keras.layers.pooling.GlobalAveragePooling2D object at 0x7f483e7f0990>\n",
      "88: <keras.layers.core.Dense object at 0x7f489e940d90>\n",
      "89: <keras.layers.core.Dropout object at 0x7f483e63d090>\n",
      "90: <keras.layers.core.Dense object at 0x7f483e7b7a50>\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "    print(\"{}: {}\".format(i, layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the range of the network \n",
    "for layer in model.layers[:87]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in model.layers[87:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15860 images belonging to 2 classes.\n",
      "Found 6795 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Stratified selection of training set and validation set, and test set\n",
    "trainDataGen = ImageDataGenerator(preprocessing_function=preprocess_input, validation_split = 0.3)\n",
    "\n",
    "# Change the batch_size\n",
    "trainGen = trainDataGen.flow_from_directory('/home/dongjun/djplace/samples/',\n",
    "                                            target_size = (224, 224),\n",
    "                                            batch_size = 64,\n",
    "                                            #color_mode = 'rgb',\n",
    "                                            class_mode = 'categorical',\n",
    "                                            subset = 'training',\n",
    "                                            shuffle = True)\n",
    "\n",
    "validGen = trainDataGen.flow_from_directory('/home/dongjun/djplace/samples/',\n",
    "                                           target_size = (224, 224),\n",
    "                                           batch_size = 64,\n",
    "                                           class_mode = 'categorical',\n",
    "                                           subset = 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = '/home/dongjun/djplace/samples/'\n",
    "EPOCHS = 2 #10, or early stopping\n",
    "LEARNINGRATE = 0.001 # decay the learning rate\n",
    "DECAYRATE = LEARNINGRATE // EPOCHS\n",
    "opt = Adam(lr = LEARNINGRATE, decay=DECAYRATE)\n",
    "\n",
    "# ADD FOCAL LOSS\n",
    "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('weights/model.hdf5', monitor = 'val_acc', mode = 'min', save_weights_only = False, save_best_only = True, verbose=1)\n",
    "tfBoard = TensorBoard(log_dir = 'logs/')\n",
    "callbacksList = [checkpoint, tfBoard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "247/247 [==============================] - 1648s 7s/step - loss: 0.0763 - accuracy: 0.9701 - val_loss: 3.4251 - val_accuracy: 0.6263\n",
      "Epoch 2/2\n",
      "247/247 [==============================] - 1639s 7s/step - loss: 0.0611 - accuracy: 0.9742 - val_loss: 3.2450 - val_accuracy: 0.6601\n"
     ]
    }
   ],
   "source": [
    "history =model.fit_generator(generator=trainGen,\n",
    "                             steps_per_epoch = trainGen.n // trainGen.batch_size,\n",
    "                             validation_data = validGen,\n",
    "                             validation_steps = validGen.n // validGen.batch_size,\n",
    "                             callbacks = callbacksList,\n",
    "                             epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = keras.models.load_model('./model.hdf5')\n",
    "def preprocessImage(img):\n",
    "    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
    "    return img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
